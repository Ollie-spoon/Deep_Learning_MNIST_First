{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This First section is where we are importing the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import winsound\n",
    "import csv\n",
    "\n",
    "##from testCases_v4a import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "\n",
    "    assert (A.shape == Z.shape)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "\n",
    "def flatten_x(train_x, test_x, standardisation_const):\n",
    "    # Reshape the training and test examples\n",
    "    train_x_flatten = train_x.reshape(train_x.shape[0], -1).T  # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "    test_x_flatten = test_x.reshape(test_x.shape[0], -1).T\n",
    "\n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    # For example for images magnitude of color is 255\n",
    "    train_x = train_x_flatten / standardisation_const\n",
    "    test_x = test_x_flatten / standardisation_const\n",
    "    return train_x, test_x\n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter\n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value\n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        a, activation_cache = sigmoid(z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        a, activation_cache = relu(z)\n",
    "\n",
    "    assert (a.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return a, cache\n",
    "\n",
    "\n",
    "def L_model_forward(x, parameters, outputs):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    a = x\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        a_prev = a\n",
    "        a, cache = linear_activation_forward(a_prev, parameters['W' + str(l)], parameters['b' + str(l)],\n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    al, cache = linear_activation_forward(a, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert (al.shape == (outputs, x.shape[1]))\n",
    "\n",
    "    return al, caches\n",
    "\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -1 / m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "\n",
    "    cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert (cost.shape == ())\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)  # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,\n",
    "                                                                                                      current_cache,\n",
    "                                                                                                      activation=\"sigmoid\")\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache,\n",
    "                                                                    activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "\n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters, layers_dims[-1])\n",
    "\n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second section is for the setup of the file import and the formating of the data from PycharmProject/DeepLearningTesting/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 63.26447939872742\n",
      "Shape of train_data(784, 60000)\n",
      "Shape of test_data(784, 10000)\n",
      "Shape of train_labels(10, 60000)\n",
      "Shape of test_labels(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "image_shape = [28, 28]\n",
    "outputs = 10\n",
    "image_pixels = image_shape[0] * image_shape[1]\n",
    "\n",
    "# Import data and format\n",
    "st = time.time()\n",
    "train_data = np.loadtxt(\"PycharmProject/DeepLearningTesting/data/mnist_train.csv\", delimiter=\",\")\n",
    "test_data = np.loadtxt(\"PycharmProject/DeepLearningTesting/data/mnist_test.csv\", delimiter=\",\")\n",
    "print(\"Time taken: \" + str(time.time() - st))\n",
    "train_labels = np.reshape([i[0] for i in train_data], (60000, 1))\n",
    "test_labels = np.reshape([i[0] for i in test_data], (10000, 1))\n",
    "\n",
    "train_labels = np.c_[train_labels, np.zeros((train_labels.shape[0], outputs - train_labels.shape[1]))]\n",
    "test_labels = np.c_[test_labels, np.zeros((test_labels.shape[0], outputs - test_labels.shape[1]))]\n",
    "\n",
    "for i in range(0, train_labels.shape[0]):\n",
    "    x = train_labels[i][0]\n",
    "    train_labels[i][0] = 0\n",
    "    train_labels[i][int(x)] = 1\n",
    "for i in range(0, test_labels.shape[0]):\n",
    "    x = test_labels[i][0]\n",
    "    test_labels[i][0] = 0\n",
    "    test_labels[i][int(x)] = 1\n",
    "train_labels = train_labels.T\n",
    "test_labels = test_labels.T\n",
    "train_data = train_data[:, 1:].T / 255 * 0.99 + 0.01\n",
    "test_data = test_data[:, 1:].T / 255 * 0.99 + 0.01\n",
    "print(\"Shape of train_data\" + str(np.shape(train_data)))\n",
    "print(\"Shape of test_data\" + str(np.shape(test_data)))\n",
    "print(\"Shape of train_labels\" + str(np.shape(train_labels)))\n",
    "print(\"Shape of test_labels\" + str(np.shape(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This thrid section is for the setup and running of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 200, 50, 10]\n",
      "Cost after iteration 0: 6.932557\n",
      "Cost after iteration 10: 5.649858\n",
      "Cost after iteration 20: 4.780002\n",
      "Cost after iteration 30: 3.307565\n",
      "Cost after iteration 40: 3.292306\n",
      "Cost after iteration 50: 3.288770\n",
      "Cost after iteration 60: 3.285754\n",
      "Cost after iteration 70: 3.283096\n",
      "Cost after iteration 80: 3.280696\n",
      "Cost after iteration 90: 3.278479\n",
      "Cost after iteration 100: 3.276391\n",
      "Time taken: 261.1825096607208\n",
      "Final cost is: 3.2763905659951313\n"
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "num_iterations = 101\n",
    "learning_rate = 0.1\n",
    "print_cost = True\n",
    "layers_dims = [image_pixels, 200, 50, outputs]\n",
    "print(layers_dims)\n",
    "\n",
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "st = time.time()\n",
    "for i in range(0, num_iterations):\n",
    "\n",
    "    # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "    AL, caches = L_model_forward(train_data, parameters, layers_dims[-1])\n",
    "\n",
    "    # Compute cost.\n",
    "    cost = compute_cost(AL, train_labels)\n",
    "\n",
    "    # Backward propagation.\n",
    "    grads = L_model_backward(AL, train_labels, caches)\n",
    "\n",
    "    # Update parameters.\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "\n",
    "    # Print the cost every 100 training example\n",
    "    if print_cost and i % 10== 0:\n",
    "        print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    if print_cost and i % 1 == 0:\n",
    "        costs.append(cost)\n",
    "\n",
    "print(\"Time taken: \" + str(time.time() - st))\n",
    "# plot the cost\n",
    "\n",
    "winsound.Beep(frequency=2500, duration=100)\n",
    "\n",
    "print(\"Final cost is: \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is to plot the cost over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#plt.yscale(\"log\")\n",
    "#plt.axis([0, len(costs), 0, 7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.90592108]\n",
      " [-0.64732214]\n",
      " [-0.88869884]\n",
      " [-0.87865486]\n",
      " [-0.8183186 ]\n",
      " [-0.84230267]\n",
      " [-0.865349  ]\n",
      " [-0.786225  ]\n",
      " [-0.9554065 ]\n",
      " [-0.84387728]]\n"
     ]
    }
   ],
   "source": [
    "print(parameters[\"b3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section is for the definition of the L_model_forward for testing (without caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_testing(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "\n",
    "    return Z\n",
    "\n",
    "\n",
    "def linear_activation_forward_testing(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        z = linear_forward_testing(A_prev, W, b)\n",
    "        a = 1/(1+np.exp(-z))\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        z = linear_forward_testing(A_prev, W, b)\n",
    "        a = np.maximum(0, z)\n",
    "\n",
    "    assert (a.shape == (W.shape[0], A_prev.shape[1]))\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def L_model_forward_testing(x, parameters, outputs):\n",
    "\n",
    "    a = x\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        a_prev = a\n",
    "        a = linear_activation_forward_testing(a_prev, parameters['W' + str(l)], parameters['b' + str(l)],\n",
    "                                             activation='relu')\n",
    "\n",
    "    al = linear_activation_forward_testing(a, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n",
    "\n",
    "    assert (al.shape == (outputs, x.shape[1]))\n",
    "\n",
    "    return al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiency with train_data 0.09751666666666667\n",
      "Efficiency with test_data 0.0974\n"
     ]
    }
   ],
   "source": [
    "def testing(X, Y, parameters, layers_dims):\n",
    "    total = 0\n",
    "    AL = L_model_forward_testing(X, parameters, layers_dims[-1])\n",
    "    AL = AL.T\n",
    "    Y = Y.T\n",
    "    for i in range(0, Y.shape[0]):\n",
    "\n",
    "        if np.where(Y[i] == np.max(Y[0]))[0][0] == np.where(AL[0] == np.max(AL[0]))[0][0]:\n",
    "            total += 1\n",
    "    return total/Y.shape[0]\n",
    "\n",
    "value = testing(train_data, train_labels, parameters, layers_dims)\n",
    "print(\"Efficiency with train_data \" + str(value))\n",
    "value = testing(test_data, test_labels, parameters, layers_dims)\n",
    "print(\"Efficiency with test_data \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
